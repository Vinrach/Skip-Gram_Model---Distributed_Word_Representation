# Distributed Representation of Words and Phrases and their Compositionality.

# Abstract
In the paper “Distributed Representations of Words and Phrases and Their Compositionality,” the 
researchers offer an improved skip-gram model for learning word vectors, which greatly advances the 
quality and speed of word representations. Key milestones consist of subsampling for frequent words 
to boost training as well as representation of infrequent words; it also introduces negative sampling as 
an alternative to hierarchical softmax. These techniques not only make training more efficient but also 
help the model capture better syntactic and semantic relations between words. Besides, another issue 
that is addressed in the paper is representing phrases and idiomatic expressions used in detecting such 
phrases and forming vector representations.

# Objective
Enhance Word Vector Quality: Improve the representation of words, particularly rare terms, by focusing on less common words during training.

Accelerate Training Process: Incorporate frequent word subsampling and negative sampling to reduce training time while maintaining accuracy.

Effective Phrase Representation: Develop a method to represent phrases as unique tokens to better understand multi-word expressions and idiomatic phrases.

Address Limitations of Initial Representations: Improve the model's ability to capture complex language structures, enabling more accurate vector representations for phrases.

Contribute to Natural Language Processing (NLP): Enhance the effectiveness and capacity of the Skip-gram model to capture a broader range of linguistic complexities.
